{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "\n",
    "## 1. Data Imbalance\n",
    "training data: 569877(96.5%) v.s. 20663(3.5%)\n",
    "\n",
    "#### 1.1 Try to balance the data:\n",
    "\n",
    "##### (Over sampling)\n",
    "\n",
    "1. simple over sampling \n",
    "2. SMOTE\n",
    "\n",
    "##### (under sampling)\n",
    "1. simple under sampling\n",
    "2. Tomek links\n",
    "\n",
    "#### 1.2 Transfer learning for rare label data\n",
    "\n",
    "## 2. large class in category features\n",
    "\n",
    "26 category features class<=10,  23 category features class>10 \n",
    "(number of class, from 3 to 13553)\n",
    "\n",
    "\n",
    "##### (if class>10, one hot encoding may suffer from dimension explode\n",
    "##### choose other encoding methods if class>10 and dis-order features)\n",
    "\n",
    "有序資料可以直接label encoding\n",
    "\n",
    "無序資料通常做one hot encoding (ex. xgboost, 例外: LGBM餵label encoding即可)\n",
    "\n",
    "## 3. plenty of NaN data\n",
    "Around 180 features get NaN datas over 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do to improve model performance\n",
    "\n",
    "### 1. deal with data imbalancing (performance variance in current work, 80-90%)\n",
    "### 2. change cross-validaiton into walk-through (for time series data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback of this project\n",
    "\n",
    "### important features:\n",
    "1. card1\n",
    "2. card2\n",
    "3. TransactionID\n",
    "4. TransactionAmt\n",
    "5. addr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ieee-fraud-detection/sample_submission.csv\n",
      "/kaggle/input/ieee-fraud-detection/train_transaction.csv\n",
      "/kaggle/input/ieee-fraud-detection/test_identity.csv\n",
      "/kaggle/input/ieee-fraud-detection/test_transaction.csv\n",
      "/kaggle/input/ieee-fraud-detection/train_identity.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "tr_id_path = '/kaggle/input/ieee-fraud-detection/train_identity.csv'\n",
    "tr_tran_path = '/kaggle/input/ieee-fraud-detection/train_transaction.csv'\n",
    "te_id_path = '/kaggle/input/ieee-fraud-detection/test_identity.csv'\n",
    "te_tran_path = '/kaggle/input/ieee-fraud-detection/test_transaction.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_id = pd.read_csv(tr_id_path)\n",
    "tr_tran = pd.read_csv(tr_tran_path)\n",
    "te_id = pd.read_csv(te_id_path)\n",
    "te_tran = pd.read_csv(te_tran_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tr_id.head)\n",
    "print (tr_tran.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.merge(tr_id, tr_tran, on=['TransactionID'], how='right')\n",
    "tr.head(10)\n",
    "\n",
    "# te = pd.merge(te_id, te_tran, on=['TransactionID'], how='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find users\n",
    "\n",
    "# tr_id_columns = tr_id.columns\n",
    "\n",
    "# x = tr_id_columns.drop('TransactionID').tolist()\n",
    "# tr_id_unique = tr.groupby(x, dropna=False).size().reset_index().rename(columns={0:'count'})\n",
    "# print (tr_id_unique)\n",
    "\n",
    "# tr_id_unique_fraud = tr[tr['isFraud']==1].groupby(x, dropna=False).size().reset_index().rename(columns={0:'count'})\n",
    "# sorted_tr_id_unique_fraud = tr_id_unique_fraud.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "# print (sorted_tr_id_unique_fraud)\n",
    "\n",
    "# print (sorted_tr_id_unique_fraud.iloc[0])\n",
    "# print (sorted_tr_id_unique_fraud.iloc[1])\n",
    "# print (sorted_tr_id_unique_fraud.iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_inf_nan(df):\n",
    "    return df.replace([np.inf, -np.inf], np.nan) \n",
    "tr = clean_inf_nan(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr[tr['isFraud']==0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr[tr['isFraud']==1].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_label = tr['isFraud']\n",
    "tr.drop(columns=[\"isFraud\"], inplace=True)\n",
    "tr.head(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tr_tran,tr_id\n",
    "del te_tran,te_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tr_label.value_counts().values\n",
    "print (x)\n",
    "print (x/sum(x)*100)\n",
    "print ('Suffer from data imbalancing')\n",
    "sns.barplot([0,1], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr['id_12'].value_counts(dropna=False, normalize=True).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### category feature processing (large class in category features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols=['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n",
    "            'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n",
    "            'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9']\n",
    "\n",
    "len_class_list = []\n",
    "label_encoder={}\n",
    "top_important_features=['card1', 'card2', 'addr1', 'P_emaildomain']  ## from results below\n",
    "for col in category_cols:\n",
    "    if col in tr.columns:\n",
    "        le=LabelEncoder()\n",
    "        le.fit(list(tr[col].astype(str).values) )  # + list(te[col].astype(str).values)\n",
    "        print (str(col) + ' ' + str(len(le.classes_))+ ' ')\n",
    "        tr[col] = le.transform(list(tr[col].astype(str).values))\n",
    "#         te[col] = le.transform(list(te[col].astype(str).values))\n",
    "        len_class_list.append(len(le.classes_))\n",
    "        \n",
    "        if col in top_important_features:\n",
    "            label_encoder[col] = le\n",
    "\n",
    "plt.hist(len_class_list, bins=100)\n",
    "plt.title('Distribution of category features class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'There are {len(len_class_list)} category label' )\n",
    "print ( f'{len([x for x in len_class_list if x>10])} category label, class >10')\n",
    "print ( f'{len([x for x in len_class_list if x<=10])} category label, class <=10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['card1']\n",
    "# tr['card2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nan processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_ratio = (tr.isna().sum())/len(tr)*100\n",
    "plt.hist(nan_ratio, bins=50)\n",
    "plt.title('Distribution of features nan_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_features_with_nan_ratio(df, nan_ratio, threshold=50):\n",
    "    features_column = nan_ratio[nan_ratio.values<threshold].index\n",
    "    return df[features_column]\n",
    "\n",
    "def fill_nan_with_mean(df, numerical_columns):\n",
    "    df[numerical_columns].fillna(df[numerical_columns].mean())\n",
    "    return df.fillna(df.mean())\n",
    "\n",
    "numerical_cols = [x for x in tr.columns if x not in category_cols]\n",
    "\n",
    "tr = exclude_features_with_nan_ratio(tr, nan_ratio)\n",
    "# tr = fill_nan_with_mean(tr, numerical_cols)\n",
    "\n",
    "tr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalize(df):\n",
    "    return (df-df.mean())/df.std()\n",
    "norm_tr = z_score_normalize(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca=PCA(n_components=2)\n",
    "# pca.fit(norm_tr)\n",
    "# pca_result = pca.transform(norm_tr)\n",
    "# plt.scatter(pca_result[:,0], pca_result[:,1], c=tr_label, s=20, alpha=1, marker='o')\n",
    "# plt.show()\n",
    "\n",
    "# pca=PCA(n_components=2)\n",
    "# pca.fit(norm_tr[numerical_cols])\n",
    "# pca_result = pca.transform(norm_tr[numerical_cols])\n",
    "# plt.scatter(pca_result[:,0], pca_result[:,1], c=tr_label, s=20, alpha=1, marker='o')\n",
    "# plt.show()\n",
    "\n",
    "# pca=PCA(n_components=2)\n",
    "# pca.fit(norm_tr[category_cols])\n",
    "# pca_result = pca.transform(norm_tr[category_cols])\n",
    "# plt.scatter(pca_result[:,0], pca_result[:,1], c=tr_label, s=20, alpha=1, marker='o')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne = TSNE(n_components=2)\n",
    "# tsne.fit(norm_tr)\n",
    "# tsne_result = tsne.transform(norm_tr)\n",
    "# tsne.scatter(tsne_result[:,0], tsne_result[:,1], c=tr_label, s=20, alpha=1, marker='o')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, val_x, train_y, val_y = train_test_split(tr, tr_label, test_size=0.2, random_state=7)\n",
    "\n",
    "# del tr, tr_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': [256],\n",
    "          'min_child_samples': [79],\n",
    "          'max_depth': [13],\n",
    "          'learning_rate': [0.03],\n",
    "          \"subsample_freq\": [3],\n",
    "          \"subsample\": [0.9],\n",
    "          \"bagging_seed\": [11],\n",
    "          \"verbosity\": [-1],\n",
    "          'reg_alpha': [0.3],\n",
    "          'reg_lambda': [0.3],\n",
    "          'colsample_bytree': [0.9],\n",
    "         }\n",
    "\n",
    "lgbm_classifier = lgb.LGBMClassifier(\n",
    "          objective= 'binary',\n",
    "          boosting_type= \"gbdt\",\n",
    "          metric= 'auc'\n",
    ")\n",
    "gsearch = GridSearchCV(lgbm_classifier, n_jobs=1, param_grid=params, cv = 5, scoring=\"roc_auc\", verbose=10)\n",
    "gsearch.fit(tr, tr_label)\n",
    "\n",
    "best_paramters = gsearch.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(gsearch.best_estimator_, max_num_features=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_non_fraud = tr[tr_label==0]\n",
    "tr_fraud = tr[tr_label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(feature, tr_non_fraud, tr_fraud, label_encoder=None, up_sampling=True):\n",
    "    non_fraud = tr_non_fraud[feature]\n",
    "    fraud = tr_fraud[feature]\n",
    "    \n",
    "    if label_encoder:\n",
    "        non_fraud= label_encoder.inverse_transform(non_fraud)\n",
    "        fraud = label_encoder.inverse_transform(fraud)\n",
    "        \n",
    "        non_fraud = np.array([x for x in non_fraud if x !='nan']).astype(float)\n",
    "        fraud = np.array([x for x in fraud if x !='nan']).astype(float)\n",
    "    \n",
    "    if up_sampling:\n",
    "        fraud = resample(fraud,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(non_fraud), # match number in majority class\n",
    "                          random_state=1) # reproducible results\n",
    "    \n",
    "    \n",
    "    plt.hist(non_fraud, bins=50)\n",
    "    plt.title(f'Distribution of non_fraud {feature}')\n",
    "    plt.hist(fraud, bins=50)\n",
    "    plt.title(f'Distribution of fraud {feature}')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    ax1=sns.kdeplot(non_fraud,shade=True,label='non_fraud')\n",
    "    ax2=sns.kdeplot(fraud,shade=True,label='fraud')\n",
    "    plt.show()\n",
    "\n",
    "    print (f'Non_fraud, mean:{np.mean(non_fraud)}, std:{np.std(non_fraud)}')\n",
    "    print (f'fraud, mean:{np.mean(fraud)}, std:{np.std(fraud)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution('card1', tr_non_fraud, tr_fraud, label_encoder=label_encoder['card1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution('card2', tr_non_fraud, tr_fraud, label_encoder=label_encoder['card2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution('addr1', tr_non_fraud, tr_fraud, label_encoder=label_encoder['addr1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution('TransactionID', tr_non_fraud, tr_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filt high TransactionAmt\n",
    "filt_tr_non_fraud = tr_non_fraud[tr_non_fraud['TransactionAmt']<1500]\n",
    "filt_tr_fraud = tr_fraud[tr_fraud['TransactionAmt']<1500]\n",
    "\n",
    "plot_distribution('TransactionAmt', filt_tr_non_fraud, filt_tr_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution('P_emaildomain', tr_non_fraud, tr_fraud)\n",
    "\n",
    "fraud_mail = tr_fraud['P_emaildomain']\n",
    "unique, counts = np.unique(fraud_mail, return_counts=True)\n",
    "print (dict(zip(unique, counts )))\n",
    "print (label_encoder['P_emaildomain'].inverse_transform([16,19,36]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
